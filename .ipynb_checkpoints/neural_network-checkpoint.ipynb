{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_params(dims):\n",
    "    \"\"\"\n",
    "    ## Generates units in each layer of the network ##\n",
    "    Inputs: number of units in each layer dims = [n0, n1, n2, ..., nL]\n",
    "    output: W = {\"W1\": W1, ..., \"WL\": WL}, b = {\"b1\": b1, ..., \"bL\": bL}\n",
    "    \"\"\"\n",
    "    # Number of layers\n",
    "    L = len(dims) - 1\n",
    "    \n",
    "    # Dictionary of parameters\n",
    "    W = {}\n",
    "    b = {}\n",
    "    for l in range(1, L+1):\n",
    "        W[\"W\"+str(l)] = np.random.randn(dims[l], dims[l-1])\n",
    "        b[\"b\"+str(l)] = np.zeros((dims[l], 1))\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "## Test initialise_params() ##\n",
      "------------------------------\n",
      "\n",
      "\n",
      "Layer 1:\n",
      "----------\n",
      "\n",
      "W1 = [[ 1.32269265  0.76733411]\n",
      " [ 0.67062741 -0.22823308]\n",
      " [ 1.73087523 -0.1064417 ]\n",
      " [-0.12818576  0.46000972]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "Layer 2:\n",
      "----------\n",
      "\n",
      "W2 = [[ 0.64942722 -0.23336502  1.41141466 -0.70446393]\n",
      " [ 0.7853863   0.61318991  0.66575737 -0.17945896]\n",
      " [ 1.01301628 -1.92160423  0.89591072  1.0470043 ]\n",
      " [ 1.15596017 -0.28235594  1.06928348 -0.02386035]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "Layer 3:\n",
      "----------\n",
      "\n",
      "W3 = [[-1.64386557  1.16212357  0.24377603 -1.33204019]\n",
      " [ 1.27421206  0.89542778 -0.10711642  0.49405832]]\n",
      "b3 = [[0.]\n",
      " [0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test initialise_params()\n",
    "dims = [2, 4, 4, 2]\n",
    "W, b = initialise_params(dims)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"## Test initialise_params() ##\")\n",
    "print(\"------------------------------\\n\\n\")\n",
    "\n",
    "for l in range(1,len(dims)):\n",
    "    print(\"Layer {}:\".format(l))\n",
    "    print(\"----------\\n\")\n",
    "    print(\"W\"+str(l)+\" = {}\".format(W[\"W\"+str(l)]))\n",
    "    print(\"b\"+str(l)+\" = {}\\n\".format(b[\"b\"+str(l)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Linear Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_linear(A_prev, W, b):\n",
    "    \"\"\"\n",
    "    ## Calculate linear equation for forward pass ##\n",
    "    Inputs: A = previous layer activation, W = current weights, b = current biases\n",
    "    Output: Z = Wâ€¢A + b\n",
    "    \"\"\"\n",
    "    \n",
    "    # Linear Equation\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "## Test forward_linear() ##\n",
      "------------------------------\n",
      "\n",
      "\n",
      "Inputs: \n",
      "----------\n",
      "x =  [[2]\n",
      " [5]]\n",
      "\n",
      "W1 =  [[ 1.32269265  0.76733411]\n",
      " [ 0.67062741 -0.22823308]\n",
      " [ 1.73087523 -0.1064417 ]\n",
      " [-0.12818576  0.46000972]]\n",
      "\n",
      "b1 =  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "Output: \n",
      "----------\n",
      "Z1 =  [[6.48205584]\n",
      " [0.20008943]\n",
      " [2.92954197]\n",
      " [2.04367705]]\n"
     ]
    }
   ],
   "source": [
    "# Test forward_linear()\n",
    "x = np.array([[2], [5]])\n",
    "W1 = W[\"W1\"]\n",
    "b1 = b[\"b1\"]\n",
    "\n",
    "Z1 = forward_linear(x, W1, b1)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"## Test forward_linear() ##\")\n",
    "print(\"------------------------------\\n\")\n",
    "\n",
    "print(\"\\nInputs: \\n----------\")\n",
    "print(\"x = \", x)\n",
    "print(\"\\nW1 = \", W1)\n",
    "print(\"\\nb1 = \", b1)\n",
    "\n",
    "print(\"\\nOutput: \\n----------\")\n",
    "print(\"Z1 = \", Z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Activation Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Activation Functions #\n",
    "########################\n",
    "    \n",
    "\n",
    "# Sigmodal Activation\n",
    "def Sigmoid(Z):\n",
    "    return 1/(1 + np.exp(-Z))\n",
    "\n",
    "# Swish Activation\n",
    "def Swish(Z):\n",
    "    return Z/(1 + np.exp(-Z))\n",
    "\n",
    "# Hyperbolic Tangent\n",
    "def Tanh(Z):\n",
    "    return np.tanh(Z)\n",
    "\n",
    "# Rectified Linear Unit\n",
    "def ReLU(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "# Leaky Rectified Linear Unit\n",
    "def L_ReLU(Z):\n",
    "    return np.maximum(0.01*Z, Z)\n",
    "\n",
    "# # Parametric Rectified Linear Unit\n",
    "# def P_ReLU(Z):\n",
    "#     return np.maximum(0.05*Z, Z)\n",
    "\n",
    "\n",
    "############################\n",
    "# Forward pass calculation #\n",
    "############################\n",
    "\n",
    "def forward_activation(Z, g):\n",
    "    \"\"\" \n",
    "    ## Applies specified activation function to Z ##\n",
    "    Inputs: Z = linear forward pass, g = activation function\n",
    "    Output: A = g(z) \n",
    "    \"\"\"\n",
    "\n",
    "    # Dictionary of activations functions\n",
    "    activations = {\n",
    "        'sigmoid': Sigmoid,\n",
    "        'swish'  : Swish,\n",
    "        'tanh'   : Tanh,\n",
    "        'relu'   : ReLU,\n",
    "        'l_relu' : L_ReLU\n",
    "    }\n",
    "\n",
    "    # Call activation on Z\n",
    "    try:\n",
    "        A = activations[g](Z)\n",
    "        return A\n",
    "    except:\n",
    "        display(print(\"\\n----------------\\nInvalid activation function\\n----------------\\n\"))\n",
    "        display(print(g))\n",
    "        display(print(\"----------------\\nValid activations:\\n----------------\"))\n",
    "        for key in activations.keys():\n",
    "            display(print(key))\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "## Test forward_activation() ##\n",
      "------------------------------\n",
      "\n",
      "\n",
      "Inputs: \n",
      "----------\n",
      "Z1 =  [[6.48205584]\n",
      " [0.20008943]\n",
      " [2.92954197]\n",
      " [2.04367705]]\n",
      "\n",
      "g =  relu\n",
      "\n",
      "Output: \n",
      "----------\n",
      "A =  [[6.48205584]\n",
      " [0.20008943]\n",
      " [2.92954197]\n",
      " [2.04367705]]\n"
     ]
    }
   ],
   "source": [
    "# Test forward_activation()\n",
    "Z1 = Z1\n",
    "g = \"relu\"\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"## Test forward_activation() ##\")\n",
    "print(\"------------------------------\\n\")\n",
    "\n",
    "print(\"\\nInputs: \\n----------\")\n",
    "print(\"Z1 = \", Z1)\n",
    "print(\"\\ng = \", g)\n",
    "\n",
    "print(\"\\nOutput: \\n----------\")\n",
    "A1 = forward_activation(Z1, g)\n",
    "print(\"A = \", A1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foward_propagation(X, W, b, g):\n",
    "    \"\"\"\n",
    "    ## Completes a full forward pass through Network ##\n",
    "    Inputs: X = Full Feature Set, W = Weights in Network, \n",
    "            b = Biases in Network, g = All Activation Functions\n",
    "    Output: A, Z\n",
    "    \"\"\"\n",
    "    assert(len(W) == len(b))\n",
    "    assert(len(b) == len(g))\n",
    "    assert(len(g) == len(W))\n",
    "    \n",
    "    L = len(W)\n",
    "    A_prev = X\n",
    "    A = {\"A0\": X}\n",
    "    Z = {}\n",
    "    \n",
    "    for l in range(1, L+1):\n",
    "        # Parameters\n",
    "        bl = b[\"b\"+str(l)]\n",
    "        Wl = W[\"W\"+str(l)]\n",
    "        \n",
    "        # Current activation \n",
    "        gl = g[\"g\"+str(l)]\n",
    "        \n",
    "        # Forward Linear Pass\n",
    "        Zl = forward_linear(A_prev, Wl, bl)\n",
    "        Al = forward_activation(Zl, gl)\n",
    "        \n",
    "        # Chase Values\n",
    "        A[\"A\"+str(l)] = Al\n",
    "        Z[\"Z\"+str(l)] = Zl\n",
    "        \n",
    "        A_prev = Al   \n",
    "    \n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "## Test forward_activation() ##\n",
      "------------------------------\n",
      "\n",
      "\n",
      "N =  [2, 4, 4, 2, 1]\n",
      "-------------------\n",
      "-------------------\n",
      "\n",
      "\n",
      "Layer 1:\n",
      "----------\n",
      "\n",
      "W1 = [[ 0.59474488  1.13412826]\n",
      " [-0.65578596  1.29851448]\n",
      " [ 0.47172899 -0.29911476]\n",
      " [-0.8651799  -0.06931088]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "Z1 = [[ 6.86013108]\n",
      " [ 5.18100047]\n",
      " [-0.55211583]\n",
      " [-2.07691418]]\n",
      "A1 = [[6.86013108]\n",
      " [5.18100047]\n",
      " [0.        ]\n",
      " [0.        ]]\n",
      "\n",
      "\n",
      "Layer 2:\n",
      "----------\n",
      "\n",
      "W2 = [[ 1.09608363 -1.73702015 -1.36216961  0.07470687]\n",
      " [-0.85613153  0.34678431 -1.59810697  0.9736059 ]\n",
      " [-0.64129417  1.44253113  0.09213193  0.23572312]\n",
      " [-0.05497847  0.41952514 -0.98894705 -0.75862592]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "Z2 = [[-1.48022481]\n",
      " [-4.07648484]\n",
      " [ 3.07439239]\n",
      " [ 1.79640043]]\n",
      "A2 = [[0.        ]\n",
      " [0.        ]\n",
      " [3.07439239]\n",
      " [1.79640043]]\n",
      "\n",
      "\n",
      "Layer 3:\n",
      "----------\n",
      "\n",
      "W3 = [[ 0.7796485   0.3134326   0.97743615 -1.59659481]\n",
      " [ 0.09390648  0.6647581  -0.85216838 -1.85154729]]\n",
      "b3 = [[0.]\n",
      " [0.]]\n",
      "\n",
      "Z3 = [[ 0.13689866]\n",
      " [-5.94602036]]\n",
      "A3 = [[0.13689866]\n",
      " [0.        ]]\n",
      "\n",
      "\n",
      "Layer 4:\n",
      "----------\n",
      "\n",
      "W4 = [[-0.76568151 -1.2816484 ]]\n",
      "b4 = [[0.]]\n",
      "\n",
      "Z4 = [[-0.10482077]]\n",
      "A4 = [[0.47381877]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test forward_propagation()\n",
    "\n",
    "## Specify inputs\n",
    "X = x\n",
    "dims = [X.shape[0], 4, 4, 2, 1]\n",
    "g = {\n",
    "    \"g1\": 'relu',\n",
    "    \"g2\": 'relu',\n",
    "    \"g3\": 'relu',\n",
    "    \"g4\": 'sigmoid'\n",
    "}\n",
    "\n",
    "## Run initialise_params()\n",
    "W, b = initialise_params(dims)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"## Test forward_activation() ##\")\n",
    "print(\"------------------------------\\n\\n\")\n",
    "\n",
    "print(\"N = \", dims)\n",
    "print(\"-------------------\\n-------------------\\n\\n\")\n",
    "\n",
    "## Run foward_propagation()\n",
    "A, Z = foward_propagation(X, W, b, g)\n",
    "\n",
    "for i in range(1,len(dims)):\n",
    "    print(\"Layer {}:\".format(i))\n",
    "    print(\"----------\\n\")\n",
    "    print(\"W\"+str(i)+\" = {}\".format(W[\"W\"+str(i)]))\n",
    "    print(\"b\"+str(i)+\" = {}\\n\".format(b[\"b\"+str(i)]))\n",
    "    print(\"Z\"+str(i)+\" = {}\".format(Z[\"Z\"+str(i)]))\n",
    "    print(\"A\"+str(i)+\" = {}\\n\\n\".format(A[\"A\"+str(i)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(AL, Y, cost_fun):\n",
    "    \"\"\"\n",
    "    Calculates the current cost (error) of the models out puts AL\n",
    "    Inputs: AL = Model Predication, Y = Expected (True) values, cost_fun = Specified Cost Function\n",
    "    Output: J = cost\n",
    "    \"\"\"\n",
    "    \n",
    "    # List of available functions\n",
    "    functions = {## Add more functionlater\n",
    "        'logistic_regression': LOG, # Logistic Regression\n",
    "        # 'mean_squared_error' : MSE, # Mean Squared Error\n",
    "        # 'mean_absolute_error': MAE  # Mean Absolute Error\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        J = functions[cost_fun](AL, Y)\n",
    "        return J\n",
    "    except:\n",
    "        display(print(\"\\n----------------\\nInvalid Cost Function\\n----------------\\n\"))\n",
    "        display(print(cost_fun))\n",
    "        display(print(\"----------------\\nValid Cost Functions:\\n----------------\"))\n",
    "        for key in activations.keys():\n",
    "            display(print(key))\n",
    "        pass\n",
    "        \n",
    "\n",
    "\n",
    "##################\n",
    "# Cost functions #\n",
    "##################\n",
    "\n",
    "# Logistic Regression\n",
    "def LOG(AL, Y):\n",
    "    J = -1/Y.shape[1]*np.sum(Y*np.log(AL) + (1-Y)*np.log(1-AL), axis=1, keepdims=True)\n",
    "    return J\n",
    "\n",
    "# Mean Squared Error\n",
    "def MSE(AL, Y):\n",
    "    error = Y - AL\n",
    "    J = -1/Y.shape[1]*np.sum(error*error, axis=1, keepdims=True)\n",
    "    return J\n",
    "\n",
    "# Mean Absolute Error\n",
    "def MAE(AL, Y):\n",
    "    J = -1/Y.shape[1]*np.sum(np.absolute(Y, AL), axis=1, keepdims=True)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "## Test cost() ##\n",
      "-------------------\n",
      "\n",
      "\n",
      "\n",
      "Inputs Test Data: \tX =  [[2]\n",
      " [5]]\n",
      "\n",
      "Expected Outputs: \tY =  [[1]]\n",
      "\n",
      "Units in each layer: \tN =  [2, 4, 4, 2, 1]\n",
      "\n",
      "Number of Layers: \tL =  4\n",
      "-------------------\n",
      "-------------------\n",
      "\n",
      "\n",
      "Layer 1:\n",
      "----------\n",
      "\n",
      "W1 = [[-0.58938394 -0.60294425]\n",
      " [-1.09702492 -0.90964251]\n",
      " [ 0.89341256  1.97183471]\n",
      " [ 0.49339128 -1.4844508 ]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "Z1 = [[-4.19348911]\n",
      " [-6.74226239]\n",
      " [11.64599869]\n",
      " [-6.43547141]]\n",
      "A1 = [[ 0.        ]\n",
      " [ 0.        ]\n",
      " [11.64599869]\n",
      " [ 0.        ]]\n",
      "\n",
      "\n",
      "Layer 2:\n",
      "----------\n",
      "\n",
      "W2 = [[-1.78492402  1.72161582 -0.19317409 -0.11620842]\n",
      " [-1.51201456 -0.5335605   0.60248921 -1.36743515]\n",
      " [-0.3527308  -0.50076353 -1.41434217  0.63628428]\n",
      " [ 0.25320603  0.27167583 -0.14382646  1.14933918]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "Z2 = [[ -2.24970515]\n",
      " [  7.0165885 ]\n",
      " [-16.47142704]\n",
      " [ -1.67500274]]\n",
      "A2 = [[0.       ]\n",
      " [7.0165885]\n",
      " [0.       ]\n",
      " [0.       ]]\n",
      "\n",
      "\n",
      "Layer 3:\n",
      "----------\n",
      "\n",
      "W3 = [[ 1.32589979  0.07078094 -0.25835739 -0.02796443]\n",
      " [ 0.07471909 -0.54647526  0.99735674  0.20050993]]\n",
      "b3 = [[0.]\n",
      " [0.]]\n",
      "\n",
      "Z3 = [[ 0.49664072]\n",
      " [-3.83439205]]\n",
      "A3 = [[0.49664072]\n",
      " [0.        ]]\n",
      "\n",
      "\n",
      "Layer 4:\n",
      "----------\n",
      "\n",
      "W4 = [[1.88469221 0.35021255]]\n",
      "b4 = [[0.]]\n",
      "\n",
      "Z4 = [[0.9360149]]\n",
      "A4 = [[0.71829398]]\n",
      "\n",
      "\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "##### Cost: J = [[0.33087635]] ####\n",
      "--------------------------------------\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test cost()\n",
    "\n",
    "## Specify inputs\n",
    "X = x\n",
    "Y = np.array([[1]])\n",
    "dims = [X.shape[0], 4, 4, 2, Y.shape[1]]\n",
    "L = len(dims) - 1\n",
    "g = {\n",
    "    \"g1\": 'relu',\n",
    "    \"g2\": 'relu',\n",
    "    \"g3\": 'relu',\n",
    "    \"g4\": 'sigmoid'\n",
    "}\n",
    "cost_fun = 'logistic_regression'\n",
    "\n",
    "## Run initialise_params()\n",
    "W, b = initialise_params(dims)\n",
    "\n",
    "print(\"-------------------\")\n",
    "print(\"## Test cost() ##\")\n",
    "print(\"-------------------\\n\\n\")\n",
    "\n",
    "print(\"\\nInputs Test Data: \\tX = \", X)\n",
    "print(\"\\nExpected Outputs: \\tY = \", Y)\n",
    "print(\"\\nUnits in each layer: \\tN = \", dims)\n",
    "print(\"\\nNumber of Layers: \\tL = \", L)\n",
    "print(\"-------------------\\n-------------------\\n\\n\")\n",
    "\n",
    "## Run foward_propagation()\n",
    "A, Z = foward_propagation(X, W, b, g)\n",
    "\n",
    "## Run cost()\n",
    "J = cost(A[\"A\"+str(L)], Y, cost_fun)\n",
    "\n",
    "for i in range(1, L+1):\n",
    "    print(\"Layer {}:\".format(i))\n",
    "    print(\"----------\\n\")\n",
    "    print(\"W\"+str(i)+\" = {}\".format(W[\"W\"+str(i)]))\n",
    "    print(\"b\"+str(i)+\" = {}\\n\".format(b[\"b\"+str(i)]))\n",
    "    print(\"Z\"+str(i)+\" = {}\".format(Z[\"Z\"+str(i)]))\n",
    "    print(\"A\"+str(i)+\" = {}\\n\\n\".format(A[\"A\"+str(i)]))\n",
    "\n",
    "print(\"--------------------------------------\\n--------------------------------------\")\n",
    "print(\"##### Cost: J = {} ####\".format(J))\n",
    "print(\"--------------------------------------\\n--------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def initalise_dAL(AL, g, cost_fun):\n",
    "#     \"\"\"\n",
    "#     Calculate dAL = dJ/dAL\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # List of available functions\n",
    "#     dFunctions = {## Add more functionlater\n",
    "#         'logistic_regression': dLOG, # Logistic Regression\n",
    "#         # 'mean_squared_error' : dMSE, # Mean Squared Error\n",
    "#         # 'mean_absolute_error': dMAE  # Mean Absolute Error\n",
    "#     }\n",
    "    \n",
    "#     J = dFunctions[cost_fun](AL, Y)\n",
    "    \n",
    "#     return dAL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test initial_dAL()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Activation Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid_prime(A):\n",
    "    return A * (1-A)\n",
    "\n",
    "def Tanh_prime(A):\n",
    "\treturn 1 - np.power(A, 2)\n",
    "\n",
    "def ReLU_prime(A):\n",
    "    return 1 if A > 0 else 0\n",
    "\n",
    "def L_ReLU_prime(A, alpha):\n",
    "\treturn 1 if A > 0 else alpha\n",
    "\n",
    "def initial_dAL(AL, Y):\n",
    "    dAL = - (Y/AL - (1 - Y)/(1 - AL))\n",
    "    \n",
    "    return dAL\n",
    "\n",
    "\n",
    "\n",
    "def backward_activation(Wl, bl, Al, dAl, A_prev, gl):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Dictionary of activations functions\n",
    "    derivatives = {\n",
    "        'sigmoid': Sigmoid_prime,\n",
    "        'tanh'   : Tanh,\n",
    "        'relu'   : ReLU_prime,\n",
    "        'l_relu' : L_ReLU_prime\n",
    "    }\n",
    "    \n",
    "    m = Al.shape[1]\n",
    "    gl_prime = derivatives[gl](Al)\n",
    "        \n",
    "    dZl = dAl*gl_prime\n",
    "        \n",
    "    dWl = 1/m*np.dot(dZl, A_prev.T)\n",
    "    \n",
    "    dbl = 1/m*np.sum(dZl, axis=1, keepdims=True)\n",
    "        \n",
    "    # Calculate dA[l-1]\n",
    "    dA_prev = np.dot(Wl.T, dZl)\n",
    "    \n",
    "    return dA_prev, dWl, dbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49664072]\n",
      " [0.        ]]\n",
      "[[0.71829398]]\n",
      "[[1.88469221 0.35021255]]\n",
      "[[0.]]\n",
      "sigmoid\n",
      "[[1]]\n",
      "[[-1.39218763]]\n",
      "[[-0.53092914]\n",
      " [-0.09865698]]\n",
      "[[-0.13990668  0.        ]]\n",
      "[[-0.28170602]]\n"
     ]
    }
   ],
   "source": [
    "# Test initial_dAL()\n",
    "A_prev = A[\"A\"+str(L-1)]\n",
    "AL = A[\"A\"+str(L)]\n",
    "gL = g[\"g\"+str(L)]\n",
    "WL = W[\"W\"+str(L)]\n",
    "bL = b[\"b\"+str(L)]\n",
    "Y = Y\n",
    "dAL = initial_dAL(AL, Y)\n",
    "\n",
    "print(A_prev)\n",
    "print(AL)\n",
    "print(WL)\n",
    "print(bL)\n",
    "print(gL)\n",
    "print(Y)\n",
    "print(dAL)\n",
    "\n",
    "dA_prev, dWL, dbL = backward_activation(WL, bL, AL, dAL, A_prev, gL)\n",
    "\n",
    "print(dA_prev)\n",
    "print(dWL)\n",
    "print(dbL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Linear Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_linear(dW, db, W, b, alpha=1):\n",
    "    \n",
    "    W -= alpha*dW\n",
    "    b -= alpha*db\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.89868288 0.35021255]]\n",
      "[[0.0281706]]\n"
     ]
    }
   ],
   "source": [
    "# Test backwards_linear()\n",
    "alpha = 0.1\n",
    "WL_updated, bL_updated= backward_linear(dWL, dbL, WL, bL, alpha)\n",
    "\n",
    "print(WL_updated)\n",
    "print(bL_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(W, b, AL, Y, cost_fun, g):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialoise\n",
    "    L = len(W)\n",
    "    A_prev = A[\"A\"+str(L)]\n",
    "    dA_prev = initial_dAL(AL, Y)\n",
    "    \n",
    "    # Backpropegate\n",
    "    for l in range(L, 0, -1):\n",
    "        # Get Variable and Parameters\n",
    "        Wl, bl, gl = W[\"W\"+str(l)], b[\"b\"+str(l)], g[\"g\"+str(l)]\n",
    "        Al, dAl = A_prev, dA_prev\n",
    "        A_prev  = A[\"A\"+str(l-1)]\n",
    "        \n",
    "        # Calculate derivates\n",
    "        dA_prv, dWl, dbl = backward_activation(WL, bL, AL, dAL, A_prev, gl)\n",
    "        \n",
    "        # Update current Parameters\n",
    "        W[\"W\"+str(l)], b[\"b\"+str(l)] = backward_linear(dWl, dbl, Wl, bl, .01)\n",
    "        \n",
    "        \n",
    "    \n",
    "    return W, b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "## Test cost() ##\n",
      "-------------------\n",
      "\n",
      "\n",
      "\n",
      "Inputs Test Data: \tX =  [[2]\n",
      " [5]]\n",
      "\n",
      "Expected Outputs: \tY =  [[1]]\n",
      "\n",
      "Units in each layer: \tN =  [2, 4, 4, 2, 1]\n",
      "\n",
      "Number of Layers: \tL =  4\n",
      "-------------------\n",
      "-------------------\n",
      "\n",
      "\n",
      "Layer 1:\n",
      "----------\n",
      "\n",
      "W1 = [[-0.5519968  -1.01310798]\n",
      " [-1.93425193  0.69680967]\n",
      " [ 0.20757714  2.39295233]\n",
      " [ 1.02715032 -0.31218374]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "Z1 = [[-6.16953349]\n",
      " [-0.38445551]\n",
      " [12.37991595]\n",
      " [ 0.49338196]]\n",
      "A1 = [[ 0.        ]\n",
      " [ 0.        ]\n",
      " [12.37991595]\n",
      " [ 0.49338196]]\n",
      "\n",
      "\n",
      "Layer 2:\n",
      "----------\n",
      "\n",
      "W2 = [[-5.70514038e-01 -1.20745117e+00  2.19027090e-01  9.21447477e-01]\n",
      " [-4.29852022e-01 -8.27169139e-01  2.01689771e+00  1.39687518e-01]\n",
      " [-1.40717569e-03  1.67871459e-01 -1.45880784e+00  5.23408636e-01]\n",
      " [-1.24248674e+00 -4.40181419e-01  1.82705749e+00  3.13422118e-01]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "Z2 = [[  3.16616253]\n",
      " [ 25.03794348]\n",
      " [-17.80167809]\n",
      " [ 22.77345502]]\n",
      "A2 = [[ 3.16616253]\n",
      " [25.03794348]\n",
      " [ 0.        ]\n",
      " [22.77345502]]\n",
      "\n",
      "\n",
      "Layer 3:\n",
      "----------\n",
      "\n",
      "W3 = [[ 1.92830576  1.9611174  -0.38960862 -0.11254697]\n",
      " [ 0.41750328  0.27230878 -2.20437066  0.62080153]]\n",
      "b3 = [[0.]\n",
      " [0.]]\n",
      "\n",
      "Z3 = [[52.6445926 ]\n",
      " [22.27773097]]\n",
      "A3 = [[52.6445926 ]\n",
      " [22.27773097]]\n",
      "\n",
      "\n",
      "Layer 4:\n",
      "----------\n",
      "\n",
      "W4 = [[0.39809875 0.15027363]]\n",
      "b4 = [[0.]]\n",
      "\n",
      "Z4 = [[24.30550184]]\n",
      "A4 = [[1.]]\n",
      "\n",
      "\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "##### Cost: J = [[2.78135293e-11]] ####\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "\n",
      "Layer 4:\n",
      "----------\n",
      "\n",
      "W4 = [[0.54640173 0.21303134]]\n",
      "b4 = [[0.00281706]]\n",
      "\n",
      "\n",
      "Layer 3:\n",
      "----------\n",
      "\n",
      "W3 = [[ 1.97238469  2.30969255 -0.38960862  0.20450225]\n",
      " [ 0.46158221  0.62088394 -2.20437066  0.93785076]]\n",
      "b3 = [[0.01392188]\n",
      " [0.01392188]]\n",
      "\n",
      "\n",
      "Layer 2:\n",
      "----------\n",
      "\n",
      "W2 = [[-5.70514038e-01 -1.20745117e+00  3.91378749e-01  9.28316279e-01]\n",
      " [-4.29852022e-01 -8.27169139e-01  2.18924937e+00  1.46556320e-01]\n",
      " [-1.40717569e-03  1.67871459e-01 -1.28645618e+00  5.30277439e-01]\n",
      " [-1.24248674e+00 -4.40181419e-01  1.99940915e+00  3.20290921e-01]]\n",
      "b2 = [[0.01392188]\n",
      " [0.01392188]\n",
      " [0.01392188]\n",
      " [0.01392188]]\n",
      "\n",
      "\n",
      "Layer 1:\n",
      "----------\n",
      "\n",
      "W1 = [[-0.52415304 -0.9434986 ]\n",
      " [-1.90640818  0.76641905]\n",
      " [ 0.23542089  2.46256172]\n",
      " [ 1.05499408 -0.24257436]]\n",
      "b1 = [[0.01392188]\n",
      " [0.01392188]\n",
      " [0.01392188]\n",
      " [0.01392188]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test back_propagation()\n",
    "\n",
    "## Specify inputs\n",
    "X = np.array([[2], [5]])\n",
    "Y = np.array([[1]])\n",
    "dims = [X.shape[0], 4, 4, 2, Y.shape[1]]\n",
    "L = len(dims) - 1\n",
    "g = {\n",
    "    \"g1\": 'relu',\n",
    "    \"g2\": 'relu',\n",
    "    \"g3\": 'relu',\n",
    "    \"g4\": 'sigmoid'\n",
    "}\n",
    "cost_fun = 'logistic_regression'\n",
    "\n",
    "## Run initialise_params()\n",
    "W, b = initialise_params(dims)\n",
    "\n",
    "print(\"-------------------\")\n",
    "print(\"## Test cost() ##\")\n",
    "print(\"-------------------\\n\\n\")\n",
    "\n",
    "print(\"\\nInputs Test Data: \\tX = \", X)\n",
    "print(\"\\nExpected Outputs: \\tY = \", Y)\n",
    "print(\"\\nUnits in each layer: \\tN = \", dims)\n",
    "print(\"\\nNumber of Layers: \\tL = \", L)\n",
    "print(\"-------------------\\n-------------------\\n\\n\")\n",
    "\n",
    "## Run foward_propagation()\n",
    "A, Z = foward_propagation(X, W, b, g)\n",
    "\n",
    "## Run cost()\n",
    "J = cost(A[\"A\"+str(L)], Y, cost_fun)\n",
    "\n",
    "for i in range(1, L+1):\n",
    "    print(\"Layer {}:\".format(i))\n",
    "    print(\"----------\\n\")\n",
    "    print(\"W\"+str(i)+\" = {}\".format(W[\"W\"+str(i)]))\n",
    "    print(\"b\"+str(i)+\" = {}\\n\".format(b[\"b\"+str(i)]))\n",
    "    print(\"Z\"+str(i)+\" = {}\".format(Z[\"Z\"+str(i)]))\n",
    "    print(\"A\"+str(i)+\" = {}\\n\\n\".format(A[\"A\"+str(i)]))\n",
    "\n",
    "print(\"--------------------------------------\\n--------------------------------------\")\n",
    "print(\"##### Cost: J = {} ####\".format(J))\n",
    "print(\"--------------------------------------\\n--------------------------------------\")\n",
    "\n",
    "W, b = back_propagation(W, b, AL, Y, cost_fun, g)\n",
    "# Print updates\n",
    "PRINT = True\n",
    "\n",
    "for l in range(L, 0, -1):\n",
    "    print(\"\\nLayer {}:\".format(l))\n",
    "    print(\"----------\\n\")\n",
    "    print(\"W\"+str(l)+\" = {}\".format(W[\"W\"+str(l)]))\n",
    "    print(\"b\"+str(l)+\" = {}\\n\".format(b[\"b\"+str(l)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
