{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from MyDenceNN import MyDenceNN\n",
    "\n",
    "random.seed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def initialise_params(dims):\n",
    "    \"\"\"\n",
    "    ## Generates units in each layer of the network ##\n",
    "    Inputs: number of units in each layer dims = [n0, n1, n2, ..., nL]\n",
    "    output: W = {\"W1\": W1, ..., \"WL\": WL}, b = {\"b1\": b1, ..., \"bL\": bL}\n",
    "    \"\"\"\n",
    "    # Number of layers\n",
    "    L = len(dims) - 1\n",
    "    \n",
    "    # Dictionary of parameters\n",
    "    W = {}\n",
    "    b = {}\n",
    "    for l in range(1, L+1):\n",
    "        W[\"W\"+str(l)] = np.random.randn(dims[l], dims[l-1])\n",
    "        b[\"b\"+str(l)] = np.zeros((dims[l], 1))\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "## Test initialise_params() ##\n",
      "------------------------------\n",
      "\n",
      "\n",
      "Layer 1:\n",
      "----------\n",
      "\n",
      "W1 = [[-0.52759779 -0.04665227]\n",
      " [-0.63599095 -0.27391402]\n",
      " [ 1.58062004 -0.52113235]\n",
      " [-1.20493073 -2.10624329]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "Layer 2:\n",
      "----------\n",
      "\n",
      "W2 = [[ 1.41669001  0.23300962  1.10227609  0.0022881 ]\n",
      " [-0.10230629  0.42934874  0.06892114 -1.84443759]\n",
      " [ 0.21352441 -0.03239448 -0.87462826 -0.27457902]\n",
      " [ 0.50235315  0.36817753 -0.70550523  1.8896441 ]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "Layer 3:\n",
      "----------\n",
      "\n",
      "W3 = [[0.26833982 1.25051952 0.60460984 0.52718498]\n",
      " [1.0078432  1.89942242 1.56969414 0.36078024]]\n",
      "b3 = [[0.]\n",
      " [0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test initialise_params()\n",
    "dims = [2, 4, 4, 2]\n",
    "W, b = initialise_params(dims)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"## Test initialise_params() ##\")\n",
    "print(\"------------------------------\\n\\n\")\n",
    "\n",
    "for l in range(1,len(dims)):\n",
    "    print(\"Layer {}:\".format(l))\n",
    "    print(\"----------\\n\")\n",
    "    print(\"W\"+str(l)+\" = {}\".format(W[\"W\"+str(l)]))\n",
    "    print(\"b\"+str(l)+\" = {}\\n\".format(b[\"b\"+str(l)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Linear Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def forward_linear(A_prev, W, b):\n",
    "    \"\"\"\n",
    "    ## Calculate linear equation for forward pass ##\n",
    "    Inputs: A = previous layer activation, W = current weights, b = current biases\n",
    "    Output: Z = Wâ€¢A + b\n",
    "    \"\"\"\n",
    "    \n",
    "    # Linear Equation\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "## Test forward_linear() ##\n",
      "------------------------------\n",
      "\n",
      "\n",
      "Inputs: \n",
      "----------\n",
      "x =  [[2]\n",
      " [5]]\n",
      "\n",
      "W1 =  [[-0.52759779 -0.04665227]\n",
      " [-0.63599095 -0.27391402]\n",
      " [ 1.58062004 -0.52113235]\n",
      " [-1.20493073 -2.10624329]]\n",
      "\n",
      "b1 =  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "Output: \n",
      "----------\n",
      "Z1 =  [[ -1.28845696]\n",
      " [ -2.641552  ]\n",
      " [  0.55557832]\n",
      " [-12.94107788]]\n"
     ]
    }
   ],
   "source": [
    "# Test forward_linear()\n",
    "x = np.array([[2], [5]])\n",
    "W1 = W[\"W1\"]\n",
    "b1 = b[\"b1\"]\n",
    "\n",
    "Z1 = forward_linear(x, W1, b1)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"## Test forward_linear() ##\")\n",
    "print(\"------------------------------\\n\")\n",
    "\n",
    "print(\"\\nInputs: \\n----------\")\n",
    "print(\"x = \", x)\n",
    "print(\"\\nW1 = \", W1)\n",
    "print(\"\\nb1 = \", b1)\n",
    "\n",
    "print(\"\\nOutput: \\n----------\")\n",
    "print(\"Z1 = \", Z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Activation Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Activation Functions #\n",
    "########################\n",
    "    \n",
    "\n",
    "# Sigmodal Activation\n",
    "def Sigmoid(Z):\n",
    "    return 1/(1 + np.exp(-Z))\n",
    "\n",
    "# Swish Activation\n",
    "def Swish(Z):\n",
    "    return Z/(1 + np.exp(-Z))\n",
    "\n",
    "# Hyperbolic Tangent\n",
    "def Tanh(Z):\n",
    "    return np.tanh(Z)\n",
    "\n",
    "# Rectified Linear Unit\n",
    "def ReLU(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "# Leaky Rectified Linear Unit\n",
    "def L_ReLU(Z):\n",
    "    return np.maximum(0.01*Z, Z)\n",
    "\n",
    "# # Parametric Rectified Linear Unit\n",
    "# def P_ReLU(Z):\n",
    "#     return np.maximum(0.05*Z, Z)\n",
    "\n",
    "\n",
    "############################\n",
    "# Forward pass calculation #\n",
    "############################\n",
    "\n",
    "def forward_activation(Z, g):\n",
    "    \"\"\" \n",
    "    ## Applies specified activation function to Z ##\n",
    "    Inputs: Z = linear forward pass, g = activation function\n",
    "    Output: A = g(z) \n",
    "    \"\"\"\n",
    "\n",
    "    # Dictionary of activations functions\n",
    "    activations = {\n",
    "        'sigmoid': Sigmoid,\n",
    "        'swish'  : Swish,\n",
    "        'tanh'   : Tanh,\n",
    "        'relu'   : ReLU,\n",
    "        'l_relu' : L_ReLU\n",
    "    }\n",
    "\n",
    "    # Call activation on Z\n",
    "    try:\n",
    "        A = activations[g](Z)\n",
    "        return A\n",
    "    except:\n",
    "        display(print(\"\\n----------------\\nInvalid activation function\\n----------------\\n\"))\n",
    "        display(print(g))\n",
    "        display(print(\"----------------\\nValid activations:\\n----------------\"))\n",
    "        for key in activations.keys():\n",
    "            display(print(key))\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "## Test forward_activation() ##\n",
      "------------------------------\n",
      "\n",
      "\n",
      "Inputs: \n",
      "----------\n",
      "Z1 =  [[ -1.28845696]\n",
      " [ -2.641552  ]\n",
      " [  0.55557832]\n",
      " [-12.94107788]]\n",
      "\n",
      "g =  relu\n",
      "\n",
      "Output: \n",
      "----------\n",
      "A =  [[0.        ]\n",
      " [0.        ]\n",
      " [0.55557832]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Test forward_activation()\n",
    "Z1 = Z1\n",
    "g = \"relu\"\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"## Test forward_activation() ##\")\n",
    "print(\"------------------------------\\n\")\n",
    "\n",
    "print(\"\\nInputs: \\n----------\")\n",
    "print(\"Z1 = \", Z1)\n",
    "print(\"\\ng = \", g)\n",
    "\n",
    "print(\"\\nOutput: \\n----------\")\n",
    "A1 = forward_activation(Z1, g)\n",
    "print(\"A = \", A1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def foward_propagation(X, W, b, g):\n",
    "    \"\"\"\n",
    "    ## Completes a full forward pass through Network ##\n",
    "    Inputs: X = Full Feature Set, W = Weights in Network, \n",
    "            b = Biases in Network, g = All Activation Functions\n",
    "    Output: A, Z\n",
    "    \"\"\"\n",
    "    assert(len(W) == len(b))\n",
    "    assert(len(b) == len(g))\n",
    "    assert(len(g) == len(W))\n",
    "    \n",
    "    L = len(W)\n",
    "    A_prev = X\n",
    "    A = {\"A0\": X}\n",
    "    Z = {}\n",
    "    \n",
    "    for l in range(1, L+1):\n",
    "        # Parameters\n",
    "        bl = b[\"b\"+str(l)]\n",
    "        Wl = W[\"W\"+str(l)]\n",
    "        \n",
    "        # Current activation \n",
    "        gl = g[\"g\"+str(l)]\n",
    "        \n",
    "        # Forward Linear Pass\n",
    "        Zl = forward_linear(A_prev, Wl, bl)\n",
    "        Al = forward_activation(Zl, gl)\n",
    "        \n",
    "        # Chase Values\n",
    "        A[\"A\"+str(l)] = Al\n",
    "        Z[\"Z\"+str(l)] = Zl\n",
    "        \n",
    "        A_prev = Al   \n",
    "    \n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "## Test forward_activation() ##\n",
      "------------------------------\n",
      "\n",
      "\n",
      "N =  [2, 4, 4, 2, 1]\n",
      "-------------------\n",
      "-------------------\n",
      "\n",
      "\n",
      "Layer 1:\n",
      "----------\n",
      "\n",
      "W1 = [[-1.67472589  1.17168787]\n",
      " [-0.73413276  0.19163704]\n",
      " [ 1.56708526  1.28891966]\n",
      " [ 0.41414088  0.08694783]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "Z1 = [[ 2.5089876 ]\n",
      " [-0.51008033]\n",
      " [ 9.57876882]\n",
      " [ 1.26302091]]\n",
      "A1 = [[2.5089876 ]\n",
      " [0.        ]\n",
      " [9.57876882]\n",
      " [1.26302091]]\n",
      "\n",
      "\n",
      "Layer 2:\n",
      "----------\n",
      "\n",
      "W2 = [[ 0.97038767 -0.39873249  0.07037556 -1.15446339]\n",
      " [ 0.30659523  0.64726699  0.08781716 -1.31573493]\n",
      " [ 0.33478087  0.61034402 -1.16348796  0.20223408]\n",
      " [-0.3366279   1.42573718  0.37038499  0.18312744]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "Z2 = [[  1.65069046]\n",
      " [ -0.05137686]\n",
      " [-10.04939529]\n",
      " [  2.93453074]]\n",
      "A2 = [[1.65069046]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [2.93453074]]\n",
      "\n",
      "\n",
      "Layer 3:\n",
      "----------\n",
      "\n",
      "W3 = [[-1.58188333  0.26071975 -0.28425539 -0.27306191]\n",
      " [-1.37581046 -0.69379836 -1.957115    2.62604198]]\n",
      "b3 = [[0.]\n",
      " [0.]]\n",
      "\n",
      "Z3 = [[-3.41250828]\n",
      " [ 5.43516372]]\n",
      "A3 = [[0.        ]\n",
      " [5.43516372]]\n",
      "\n",
      "\n",
      "Layer 4:\n",
      "----------\n",
      "\n",
      "W4 = [[-0.19585305  0.08847368]]\n",
      "b4 = [[0.]]\n",
      "\n",
      "Z4 = [[0.48086895]]\n",
      "A4 = [[0.61795304]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test forward_propagation()\n",
    "\n",
    "## Specify inputs\n",
    "X = x\n",
    "dims = [X.shape[0], 4, 4, 2, 1]\n",
    "g = {\n",
    "    \"g1\": 'relu',\n",
    "    \"g2\": 'relu',\n",
    "    \"g3\": 'relu',\n",
    "    \"g4\": 'sigmoid'\n",
    "}\n",
    "\n",
    "## Run initialise_params()\n",
    "W, b = initialise_params(dims)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"## Test forward_activation() ##\")\n",
    "print(\"------------------------------\\n\\n\")\n",
    "\n",
    "print(\"N = \", dims)\n",
    "print(\"-------------------\\n-------------------\\n\\n\")\n",
    "\n",
    "## Run foward_propagation()\n",
    "A, Z = foward_propagation(X, W, b, g)\n",
    "\n",
    "for i in range(1,len(dims)):\n",
    "    print(\"Layer {}:\".format(i))\n",
    "    print(\"----------\\n\")\n",
    "    print(\"W\"+str(i)+\" = {}\".format(W[\"W\"+str(i)]))\n",
    "    print(\"b\"+str(i)+\" = {}\\n\".format(b[\"b\"+str(i)]))\n",
    "    print(\"Z\"+str(i)+\" = {}\".format(Z[\"Z\"+str(i)]))\n",
    "    print(\"A\"+str(i)+\" = {}\\n\\n\".format(A[\"A\"+str(i)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def cost(AL, Y, cost_fun):\n",
    "    \"\"\"\n",
    "    Calculates the current cost (error) of the models out puts AL\n",
    "    Inputs: AL = Model Predication, Y = Expected (True) values, cost_fun = Specified Cost Function\n",
    "    Output: J = cost\n",
    "    \"\"\"\n",
    "    \n",
    "    # List of available functions\n",
    "    functions = {## Add more functionlater\n",
    "        'logistic_regression': LOG, # Logistic Regression\n",
    "        # 'mean_squared_error' : MSE, # Mean Squared Error\n",
    "        # 'mean_absolute_error': MAE  # Mean Absolute Error\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        J = functions[cost_fun](AL, Y)\n",
    "        return J\n",
    "    except:\n",
    "        display(print(\"\\n----------------\\nInvalid Cost Function\\n----------------\\n\"))\n",
    "        display(print(cost_fun))\n",
    "        display(print(\"----------------\\nValid Cost Functions:\\n----------------\"))\n",
    "        for key in activations.keys():\n",
    "            display(print(key))\n",
    "        pass\n",
    "        \n",
    "\n",
    "\n",
    "##################\n",
    "# Cost functions #\n",
    "##################\n",
    "\n",
    "# Logistic Regression\n",
    "def LOG(AL, Y):\n",
    "    J = -1/Y.shape[1]*np.sum(Y*np.log(AL) + (1-Y)*np.log(1-AL), axis=1, keepdims=True)\n",
    "    return J\n",
    "\n",
    "# Mean Squared Error\n",
    "def MSE(AL, Y):\n",
    "    error = Y - AL\n",
    "    J = -1/Y.shape[1]*np.sum(error*error, axis=1, keepdims=True)\n",
    "    return J\n",
    "\n",
    "# Mean Absolute Error\n",
    "def MAE(AL, Y):\n",
    "    J = -1/Y.shape[1]*np.sum(np.absolute(Y, AL), axis=1, keepdims=True)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "## Test cost() ##\n",
      "-------------------\n",
      "\n",
      "\n",
      "\n",
      "Inputs Test Data: \tX =  [[2]\n",
      " [5]]\n",
      "\n",
      "Expected Outputs: \tY =  [[1]]\n",
      "\n",
      "Units in each layer: \tN =  [2, 4, 4, 2, 1]\n",
      "\n",
      "Number of Layers: \tL =  4\n",
      "-------------------\n",
      "-------------------\n",
      "\n",
      "\n",
      "Layer 1:\n",
      "----------\n",
      "\n",
      "W1 = [[-0.21901098 -0.45797075]\n",
      " [-0.87272402 -1.869352  ]\n",
      " [-1.8785267  -0.42425082]\n",
      " [-0.37559362  0.07861534]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "Z1 = [[ -2.72787571]\n",
      " [-11.09220806]\n",
      " [ -5.87830752]\n",
      " [ -0.35811053]]\n",
      "A1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "Layer 2:\n",
      "----------\n",
      "\n",
      "W2 = [[ 0.76364955  0.18902406 -1.29198788 -0.0217373 ]\n",
      " [ 1.65281356 -1.25447408 -0.53294204  0.7844104 ]\n",
      " [ 1.26578813  0.35131684  0.10182487  0.91627151]\n",
      " [ 1.4131937  -1.04739154 -0.12172404  0.98545104]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "Z2 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "A2 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "Layer 3:\n",
      "----------\n",
      "\n",
      "W3 = [[ 0.95678973  0.27632893  0.22575616  0.22424986]\n",
      " [-0.8731264   0.33624783 -1.02009524  0.38222052]]\n",
      "b3 = [[0.]\n",
      " [0.]]\n",
      "\n",
      "Z3 = [[0.]\n",
      " [0.]]\n",
      "A3 = [[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "Layer 4:\n",
      "----------\n",
      "\n",
      "W4 = [[ 1.20823389 -1.91686432]]\n",
      "b4 = [[0.]]\n",
      "\n",
      "Z4 = [[0.]]\n",
      "A4 = [[0.5]]\n",
      "\n",
      "\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "##### Cost: J = [[0.69314718]] ####\n",
      "--------------------------------------\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test cost()\n",
    "\n",
    "## Specify inputs\n",
    "X = x\n",
    "Y = np.array([[1]])\n",
    "dims = [X.shape[0], 4, 4, 2, Y.shape[1]]\n",
    "L = len(dims) - 1\n",
    "g = {\n",
    "    \"g1\": 'relu',\n",
    "    \"g2\": 'relu',\n",
    "    \"g3\": 'relu',\n",
    "    \"g4\": 'sigmoid'\n",
    "}\n",
    "cost_fun = 'logistic_regression'\n",
    "\n",
    "## Run initialise_params()\n",
    "W, b = initialise_params(dims)\n",
    "\n",
    "print(\"-------------------\")\n",
    "print(\"## Test cost() ##\")\n",
    "print(\"-------------------\\n\\n\")\n",
    "\n",
    "print(\"\\nInputs Test Data: \\tX = \", X)\n",
    "print(\"\\nExpected Outputs: \\tY = \", Y)\n",
    "print(\"\\nUnits in each layer: \\tN = \", dims)\n",
    "print(\"\\nNumber of Layers: \\tL = \", L)\n",
    "print(\"-------------------\\n-------------------\\n\\n\")\n",
    "\n",
    "## Run foward_propagation()\n",
    "A, Z = foward_propagation(X, W, b, g)\n",
    "\n",
    "## Run cost()\n",
    "J = cost(A[\"A\"+str(L)], Y, cost_fun)\n",
    "\n",
    "for i in range(1, L+1):\n",
    "    print(\"Layer {}:\".format(i))\n",
    "    print(\"----------\\n\")\n",
    "    print(\"W\"+str(i)+\" = {}\".format(W[\"W\"+str(i)]))\n",
    "    print(\"b\"+str(i)+\" = {}\\n\".format(b[\"b\"+str(i)]))\n",
    "    print(\"Z\"+str(i)+\" = {}\".format(Z[\"Z\"+str(i)]))\n",
    "    print(\"A\"+str(i)+\" = {}\\n\\n\".format(A[\"A\"+str(i)]))\n",
    "\n",
    "print(\"--------------------------------------\\n--------------------------------------\")\n",
    "print(\"##### Cost: J = {} ####\".format(J))\n",
    "print(\"--------------------------------------\\n--------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def initalise_dAL(AL, g, cost_fun):\n",
    "#     \"\"\"\n",
    "#     Calculate dAL = dJ/dAL\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # List of available functions\n",
    "#     dFunctions = {## Add more functionlater\n",
    "#         'logistic_regression': dLOG, # Logistic Regression\n",
    "#         # 'mean_squared_error' : dMSE, # Mean Squared Error\n",
    "#         # 'mean_absolute_error': dMAE  # Mean Absolute Error\n",
    "#     }\n",
    "    \n",
    "#     J = dFunctions[cost_fun](AL, Y)\n",
    "    \n",
    "#     return dAL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Test initial_dAL()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Activation Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def Sigmoid_prime(A):\n",
    "    return A * (1-A)\n",
    "\n",
    "def Tanh_prime(A):\n",
    "\treturn 1 - np.power(A, 2)\n",
    "\n",
    "def ReLU_prime(A):\n",
    "    return 1 if A > 0 else 0\n",
    "\n",
    "def L_ReLU_prime(A, alpha):\n",
    "\treturn 1 if A > 0 else alpha\n",
    "\n",
    "def initial_dAL(AL, Y):\n",
    "    dAL = - (Y/AL - (1 - Y)/(1 - AL))\n",
    "    \n",
    "    return dAL\n",
    "\n",
    "\n",
    "\n",
    "def backward_activation(Wl, bl, Al, dAl, A_prev, gl):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Dictionary of activations functions\n",
    "    derivatives = {\n",
    "        'sigmoid': Sigmoid_prime,\n",
    "        'tanh'   : Tanh,\n",
    "        'relu'   : ReLU_prime,\n",
    "        'l_relu' : L_ReLU_prime\n",
    "    }\n",
    "    \n",
    "    m = Al.shape[1]\n",
    "    gl_prime = derivatives[gl](Al)\n",
    "        \n",
    "    dZl = dAl*gl_prime\n",
    "        \n",
    "    dWl = 1/m*np.dot(dZl, A_prev.T)\n",
    "    \n",
    "    dbl = 1/m*np.sum(dZl, axis=1, keepdims=True)\n",
    "        \n",
    "    # Calculate dA[l-1]\n",
    "    dA_prev = np.dot(Wl.T, dZl)\n",
    "    \n",
    "    return dA_prev, dWl, dbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]]\n",
      "[[0.5]]\n",
      "[[ 1.20823389 -1.91686432]]\n",
      "[[0.]]\n",
      "sigmoid\n",
      "[[1]]\n",
      "[[-2.]]\n",
      "[[-0.60411695]\n",
      " [ 0.95843216]]\n",
      "[[0. 0.]]\n",
      "[[-0.5]]\n"
     ]
    }
   ],
   "source": [
    "# Test initial_dAL()\n",
    "A_prev = A[\"A\"+str(L-1)]\n",
    "AL = A[\"A\"+str(L)]\n",
    "gL = g[\"g\"+str(L)]\n",
    "WL = W[\"W\"+str(L)]\n",
    "bL = b[\"b\"+str(L)]\n",
    "Y = Y\n",
    "dAL = initial_dAL(AL, Y)\n",
    "\n",
    "print(A_prev)\n",
    "print(AL)\n",
    "print(WL)\n",
    "print(bL)\n",
    "print(gL)\n",
    "print(Y)\n",
    "print(dAL)\n",
    "\n",
    "dA_prev, dWL, dbL = backward_activation(WL, bL, AL, dAL, A_prev, gL)\n",
    "\n",
    "print(dA_prev)\n",
    "print(dWL)\n",
    "print(dbL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Linear Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def backward_linear(dW, db, W, b, alpha=1):\n",
    "    \n",
    "    W -= alpha*dW\n",
    "    b -= alpha*db\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.20823389 -1.91686432]]\n",
      "[[0.05]]\n"
     ]
    }
   ],
   "source": [
    "# Test backwards_linear()\n",
    "alpha = 0.1\n",
    "WL_updated, bL_updated= backward_linear(dWL, dbL, WL, bL, alpha)\n",
    "\n",
    "print(WL_updated)\n",
    "print(bL_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def back_propagation(W, b, AL, Y, cost_fun, g):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialoise\n",
    "    L = len(W)\n",
    "    A_prev = A[\"A\"+str(L)]\n",
    "    dA_prev = initial_dAL(AL, Y)\n",
    "    \n",
    "    # Backpropegate\n",
    "    for l in range(L, 0, -1):\n",
    "        # Get Variable and Parameters\n",
    "        Wl, bl, gl = W[\"W\"+str(l)], b[\"b\"+str(l)], g[\"g\"+str(l)]\n",
    "        Al, dAl = A_prev, dA_prev\n",
    "        A_prev  = A[\"A\"+str(l-1)]\n",
    "        \n",
    "        # Calculate derivates\n",
    "        dA_prv, dWl, dbl = backward_activation(WL, bL, AL, dAL, A_prev, gl)\n",
    "        \n",
    "        # Update current Parameters\n",
    "        W[\"W\"+str(l)], b[\"b\"+str(l)] = backward_linear(dWl, dbl, Wl, bl, .01)\n",
    "        \n",
    "        \n",
    "    \n",
    "    return W, b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "## Test cost() ##\n",
      "-------------------\n",
      "\n",
      "\n",
      "\n",
      "Inputs Test Data: \tX =  [[2]\n",
      " [5]]\n",
      "\n",
      "Expected Outputs: \tY =  [[1]]\n",
      "\n",
      "Units in each layer: \tN =  [2, 4, 4, 2, 1]\n",
      "\n",
      "Number of Layers: \tL =  4\n",
      "-------------------\n",
      "-------------------\n",
      "\n",
      "\n",
      "Layer 1:\n",
      "----------\n",
      "\n",
      "W1 = [[ 1.49116479 -2.21504256]\n",
      " [-0.17000539 -0.80244003]\n",
      " [ 1.60671005  0.10759652]\n",
      " [-0.43025835  0.35524553]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "Z1 = [[-8.09288323]\n",
      " [-4.35221095]\n",
      " [ 3.75140269]\n",
      " [ 0.91571094]]\n",
      "A1 = [[0.        ]\n",
      " [0.        ]\n",
      " [3.75140269]\n",
      " [0.91571094]]\n",
      "\n",
      "\n",
      "Layer 2:\n",
      "----------\n",
      "\n",
      "W2 = [[ 0.62122217  0.81608305  1.44128942 -0.63473784]\n",
      " [ 0.30913332 -0.00746083  1.73183946 -0.77769518]\n",
      " [-1.02737469  0.49502795 -0.13817145 -0.66329786]\n",
      " [-0.30714873 -1.22141762  0.26524205  0.36767423]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "Z2 = [[ 4.82562062]\n",
      " [ 5.78468322]\n",
      " [-1.12572585]\n",
      " [ 1.33171307]]\n",
      "A2 = [[4.82562062]\n",
      " [5.78468322]\n",
      " [0.        ]\n",
      " [1.33171307]]\n",
      "\n",
      "\n",
      "Layer 3:\n",
      "----------\n",
      "\n",
      "W3 = [[-0.11741849  0.06480004 -0.53672602  0.48352786]\n",
      " [ 0.54129329  1.41901178 -1.87146593  0.28370912]]\n",
      "b3 = [[0.]\n",
      " [0.]]\n",
      "\n",
      "Z3 = [[ 0.45215095]\n",
      " [11.19842882]]\n",
      "A3 = [[ 0.45215095]\n",
      " [11.19842882]]\n",
      "\n",
      "\n",
      "Layer 4:\n",
      "----------\n",
      "\n",
      "W4 = [[ 2.05975382 -0.46871442]]\n",
      "b4 = [[0.]]\n",
      "\n",
      "Z4 = [[-4.31754538]]\n",
      "A4 = [[0.01315715]]\n",
      "\n",
      "\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "##### Cost: J = [[4.33078986]] ####\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "\n",
      "Layer 4:\n",
      "----------\n",
      "\n",
      "W4 = [[ 2.06201458 -0.41272227]]\n",
      "b4 = [[0.005]]\n",
      "\n",
      "\n",
      "Layer 3:\n",
      "----------\n",
      "\n",
      "W3 = [[-0.02090608  0.1804937  -0.53672602  0.51016212]\n",
      " [ 0.6378057   1.53470545 -1.87146593  0.31034338]]\n",
      "b3 = [[0.02]\n",
      " [0.02]]\n",
      "\n",
      "\n",
      "Layer 2:\n",
      "----------\n",
      "\n",
      "W2 = [[ 0.62122217  0.81608305  1.51631747 -0.61642362]\n",
      " [ 0.30913332 -0.00746083  1.80686751 -0.75938096]\n",
      " [-1.02737469  0.49502795 -0.06314339 -0.64498364]\n",
      " [-0.30714873 -1.22141762  0.34027011  0.38598844]]\n",
      "b2 = [[0.02]\n",
      " [0.02]\n",
      " [0.02]\n",
      " [0.02]]\n",
      "\n",
      "\n",
      "Layer 1:\n",
      "----------\n",
      "\n",
      "W1 = [[ 1.53116479 -2.11504256]\n",
      " [-0.13000539 -0.70244003]\n",
      " [ 1.64671005  0.20759652]\n",
      " [-0.39025835  0.45524553]]\n",
      "b1 = [[0.02]\n",
      " [0.02]\n",
      " [0.02]\n",
      " [0.02]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test back_propagation()\n",
    "\n",
    "## Specify inputs\n",
    "X = np.array([[2], [5]])\n",
    "Y = np.array([[1]])\n",
    "dims = [X.shape[0], 4, 4, 2, Y.shape[1]]\n",
    "L = len(dims) - 1\n",
    "g = {\n",
    "    \"g1\": 'relu',\n",
    "    \"g2\": 'relu',\n",
    "    \"g3\": 'relu',\n",
    "    \"g4\": 'sigmoid'\n",
    "}\n",
    "cost_fun = 'logistic_regression'\n",
    "\n",
    "## Run initialise_params()\n",
    "W, b = initialise_params(dims)\n",
    "\n",
    "print(\"-------------------\")\n",
    "print(\"## Test cost() ##\")\n",
    "print(\"-------------------\\n\\n\")\n",
    "\n",
    "print(\"\\nInputs Test Data: \\tX = \", X)\n",
    "print(\"\\nExpected Outputs: \\tY = \", Y)\n",
    "print(\"\\nUnits in each layer: \\tN = \", dims)\n",
    "print(\"\\nNumber of Layers: \\tL = \", L)\n",
    "print(\"-------------------\\n-------------------\\n\\n\")\n",
    "\n",
    "## Run foward_propagation()\n",
    "A, Z = foward_propagation(X, W, b, g)\n",
    "\n",
    "## Run cost()\n",
    "J = cost(A[\"A\"+str(L)], Y, cost_fun)\n",
    "\n",
    "for i in range(1, L+1):\n",
    "    print(\"Layer {}:\".format(i))\n",
    "    print(\"----------\\n\")\n",
    "    print(\"W\"+str(i)+\" = {}\".format(W[\"W\"+str(i)]))\n",
    "    print(\"b\"+str(i)+\" = {}\\n\".format(b[\"b\"+str(i)]))\n",
    "    print(\"Z\"+str(i)+\" = {}\".format(Z[\"Z\"+str(i)]))\n",
    "    print(\"A\"+str(i)+\" = {}\\n\\n\".format(A[\"A\"+str(i)]))\n",
    "\n",
    "print(\"--------------------------------------\\n--------------------------------------\")\n",
    "print(\"##### Cost: J = {} ####\".format(J))\n",
    "print(\"--------------------------------------\\n--------------------------------------\")\n",
    "\n",
    "W, b = back_propagation(W, b, AL, Y, cost_fun, g)\n",
    "# Print updates\n",
    "PRINT = True\n",
    "\n",
    "for l in range(L, 0, -1):\n",
    "    print(\"\\nLayer {}:\".format(l))\n",
    "    print(\"----------\\n\")\n",
    "    print(\"W\"+str(l)+\" = {}\".format(W[\"W\"+str(l)]))\n",
    "    print(\"b\"+str(l)+\" = {}\\n\".format(b[\"b\"+str(l)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
